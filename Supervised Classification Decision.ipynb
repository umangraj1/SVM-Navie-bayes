{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04853dcb",
      "metadata": {
        "id": "04853dcb"
      },
      "source": [
        "### Question 1: What is Information Gain, and how is it used in Decision Trees?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be1b803",
      "metadata": {
        "id": "7be1b803"
      },
      "source": [
        "\n",
        "**Answer:**  \n",
        "Information Gain (IG) is a metric used to measure the effectiveness of an attribute in classifying data.  \n",
        "It is based on the concept of **entropy**, which measures the impurity or disorder of a dataset.\n",
        "\n",
        "The formula for Information Gain is:  \n",
        "\n",
        "\\`\\`\\`\n",
        "Information Gain = Entropy(Parent) - [Weighted Average] * Entropy(Children)\n",
        "\\`\\`\\`\n",
        "\n",
        "- A higher Information Gain means the attribute provides more information about the target class.\n",
        "- Decision Trees use IG to decide which feature to split on at each node — choosing the one with the **highest IG**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d3610b",
      "metadata": {
        "id": "f3d3610b"
      },
      "source": [
        "### Question 2: What is the difference between Gini Impurity and Entropy?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3cab79",
      "metadata": {
        "id": "5e3cab79"
      },
      "source": [
        "\n",
        "**Answer:**  \n",
        "| Metric | Formula | Range | Interpretation |\n",
        "|---------|----------|--------|----------------|\n",
        "| **Entropy** | \\(-p_1 \\log_2 p_1 - p_2 \\log_2 p_2 - ... - p_n \\log_2 p_n\\) | 0–1 | Measures impurity based on information theory |\n",
        "| **Gini Impurity** | \\(1 - \\sum p_i^2\\) | 0–0.5 | Measures how often a randomly chosen element is incorrectly labeled |\n",
        "\n",
        "**Key Differences:**\n",
        "- Entropy involves logarithmic computation (slower but information-theoretic).\n",
        "- Gini is simpler and faster to compute.\n",
        "- Both yield similar trees, but Gini often performs better in practice for large datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3cec1cc",
      "metadata": {
        "id": "e3cec1cc"
      },
      "source": [
        "### Question 3: What is Pre-Pruning in Decision Trees?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d22b30ab",
      "metadata": {
        "id": "d22b30ab"
      },
      "source": [
        "\n",
        "**Answer:**  \n",
        "Pre-Pruning (also called **Early Stopping**) prevents a Decision Tree from becoming too complex.  \n",
        "Instead of fully growing the tree and pruning later, it stops the growth early when further splits don't significantly improve performance.\n",
        "\n",
        "**Common Pre-Pruning Criteria:**\n",
        "- Maximum depth (`max_depth`)\n",
        "- Minimum samples per leaf (`min_samples_leaf`)\n",
        "- Minimum information gain threshold\n",
        "\n",
        "This helps reduce **overfitting** and improves **generalization**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba57d2ac",
      "metadata": {
        "id": "ba57d2ac"
      },
      "source": [
        "### Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1dcfab3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dcfab3f",
        "outputId": "b1da6088-14d2-4051-f801-87d9d80bf2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "petal length (cm)    0.564056\n",
            "petal width (cm)     0.422611\n",
            "sepal length (cm)    0.013333\n",
            "sepal width (cm)     0.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Decision Tree using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Feature importances\n",
        "importances = pd.Series(clf.feature_importances_, index=iris.feature_names)\n",
        "print(\"Feature Importances:\")\n",
        "print(importances.sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2d5087",
      "metadata": {
        "id": "ca2d5087"
      },
      "source": [
        "### Question 5: What is a Support Vector Machine (SVM)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db6e5ee",
      "metadata": {
        "id": "2db6e5ee"
      },
      "source": [
        "\n",
        "**Answer:**  \n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.  \n",
        "It finds the **optimal hyperplane** that best separates data points of different classes with **maximum margin**.\n",
        "\n",
        "Key concepts:\n",
        "- **Support Vectors:** Data points closest to the decision boundary.\n",
        "- **Margin:** Distance between the hyperplane and support vectors.\n",
        "- **Goal:** Maximize the margin for better generalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a375ca3c",
      "metadata": {
        "id": "a375ca3c"
      },
      "source": [
        "### Question 6: What is the Kernel Trick in SVM?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a12940d9",
      "metadata": {
        "id": "a12940d9"
      },
      "source": [
        "\n",
        "**Answer:**  \n",
        "The **Kernel Trick** allows SVM to perform nonlinear classification efficiently.  \n",
        "It maps data into a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "Common kernels:\n",
        "- **Linear:** Suitable for linearly separable data.\n",
        "- **Polynomial:** Adds interaction features.\n",
        "- **RBF (Radial Basis Function):** Handles complex, nonlinear boundaries.\n",
        "\n",
        "Formula example for RBF:  \n",
        "\\( K(x, x') = \\exp(-\\gamma ||x - x'||^2) \\)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a89dc7",
      "metadata": {
        "id": "c7a89dc7"
      },
      "source": [
        "### Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b9146176",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9146176",
        "outputId": "a9c2f431-774c-45e0-adf8-ac8d9cb120e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0000\n",
            "RBF Kernel Accuracy: 0.8056\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {linear_acc:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {rbf_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2b8c89",
      "metadata": {
        "id": "db2b8c89"
      },
      "source": [
        "### Question 8: What is the Naïve Bayes classifier, and why is it called 'Naïve'?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d72570e",
      "metadata": {
        "id": "6d72570e"
      },
      "source": [
        "\n",
        "**Answer:**  \n",
        "Naïve Bayes is a **probabilistic classifier** based on **Bayes' Theorem**, assuming independence between features.\n",
        "\n",
        "Formula:  \n",
        "\\( P(C|X) = \\frac{P(X|C) * P(C)}{P(X)} \\)\n",
        "\n",
        "It’s called “Naïve” because it **assumes all features are independent**, which is rarely true in practice, but still performs surprisingly well for many problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5aa22e5",
      "metadata": {
        "id": "c5aa22e5"
      },
      "source": [
        "### Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa369da",
      "metadata": {
        "id": "4fa369da"
      },
      "source": [
        "\n",
        "**Answer:**  \n",
        "| Type | Data Type | Use Case | Distribution |\n",
        "|------|------------|----------|---------------|\n",
        "| **GaussianNB** | Continuous | Real-valued features (e.g., Iris, Breast Cancer) | Normal distribution |\n",
        "| **MultinomialNB** | Discrete | Text classification (word counts) | Multinomial distribution |\n",
        "| **BernoulliNB** | Binary | Binary/boolean features (e.g., presence/absence of word) | Bernoulli distribution |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1cace1",
      "metadata": {
        "id": "0c1cace1"
      },
      "source": [
        "### Question 10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "57a087bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57a087bd",
        "outputId": "49aac0f9-bf03-4165-c490-281c1b429858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GaussianNB Accuracy: 0.9737\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"GaussianNB Accuracy: {accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}